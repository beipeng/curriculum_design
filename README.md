# curriculum_design

This is an user study designed to investigate 1) whether non-expert humans can design a good curriculum of tasks to help an agent learn some given target task in a sequential decision domain, 2) what kind of bias do they have when designing curriculum, and 3) how to modify our curriculum-learning algorithm to better handle human-generated curricula. 

To enable this, we used our own Sokoban-like test domain. In this domain, each task is specified by a text command, and the agent is trained to perform the task via reward and punishment. In our user study, participants were asked to select a sequence of training tasks for the virtual dog to help it quickly learn to complete the final target task. Before the experiments, participants had to pass a color blindness test, after which they completed a background survey regarding their prior experience in training dogs. Participants then went through a tutorial that 1) walked them through two examples of the dog being trained to help them understand how the dog learns to perform a command from positive and negative feedback, and 2) taught them how to use the interface to design a curriculum. Participants were told ?In this study, your goal is to design a curriculum (a set of assignments) for a virtual dog to train on, so that the dog can quickly completethe target assignment.? They were also told that they could observe the process of the dog being trained on each task in their curriculum, including the target task, and that they would receive a bonus on top of their base compensation, depending on the quality of the curriculum they designed.

Following the tutorial, participants began the experiment itself, in which they selected environments and commands from a 16-environment grid in any order they wished to design a curriculum. Participants were required to include at least one source task in their curricula, but there was no upper limit on how long the curricula could be, and repeated tasks were allowed. The target task was always shown on the right side of the screen to remind the participant of what the agent ultimately needed to learn. Once a participant finished their initial curriculum, they were shown the agent being trained on that curriculum, after which they were given the opportunity to redesign it to improve the agent?s performance. Participants were required to redesign their curriculum at least once before making it their finalsubmission, but could redesign it (and observe the training process) as many times as they wished before submitting. It's worth noting that in the user study, participants did not receive any explicit feedback regarding the quality of the curriculum they designed. 

Since the server we used before for this user study is not maintained anymore, when you run this user study, you might not be able to go through the evaluation process, where you are supposed to be able to watch the agent being trained on the curriculum you designed. But overall, it should be able to give you an idea of what our user study looks like. 